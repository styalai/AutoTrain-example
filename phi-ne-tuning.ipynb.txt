{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-30T11:39:38.297284Z","iopub.execute_input":"2024-04-30T11:39:38.297703Z","iopub.status.idle":"2024-04-30T11:39:38.305419Z","shell.execute_reply.started":"2024-04-30T11:39:38.297662Z","shell.execute_reply":"2024-04-30T11:39:38.304340Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os\n!pip install -U autotrain-advanced > install_logs.txt\n!autotrain setup --colab > setup_logs.txt","metadata":{"execution":{"iopub.status.idle":"2024-04-30T10:56:58.070939Z","shell.execute_reply.started":"2024-04-30T10:52:17.791400Z","shell.execute_reply":"2024-04-30T10:56:58.069715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\nimport pandas as pd\ndata = {\"text\":[]}\n\n# addition\nfor i in range(100):\n    nbr1 = random.randint(1, 100)\n    nbr2 = random.randint(1, 100)\n    \n    user = \"<|user|>\"+str(nbr1)+\"+\"+str(nbr2)+\"<|end|>\\n\"\n    phi = \"<|assistant|>\"+str(nbr1)+\"+\"+str(nbr2)+\"=\"+str(nbr1+nbr2)+\"<|end|>\"\n    prompt = user+phi\n    data[\"text\"].append(prompt)\n# multiplication\nfor i in range(150):\n    nbr1 = random.randint(1, 10)\n    nbr2 = random.randint(1, 10)\n    \n    user = \"<|user|>\" + str(nbr1)+\"*\"+str(nbr2) + \"<|end|>\\n\"\n    phi = \"<|assistant|>\" + str(nbr1)+\"*\"+str(nbr2)+\"=\"+str(nbr1*nbr2)+\"<|end|>\"\n    prompt = user+phi\n    data[\"text\"].append(prompt)\n\ndata = pd.DataFrame.from_dict(data)\n#os.mkdir(\"/kaggle/working/data\")\ndata.to_csv('data/dataset.csv', index=False)  ","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:50:45.439051Z","iopub.execute_input":"2024-04-30T11:50:45.439858Z","iopub.status.idle":"2024-04-30T11:50:45.456316Z","shell.execute_reply.started":"2024-04-30T11:50:45.439814Z","shell.execute_reply":"2024-04-30T11:50:45.455089Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('data/dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:50:47.213700Z","iopub.execute_input":"2024-04-30T11:50:47.214838Z","iopub.status.idle":"2024-04-30T11:50:47.232399Z","shell.execute_reply.started":"2024-04-30T11:50:47.214801Z","shell.execute_reply":"2024-04-30T11:50:47.231299Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                                                  text\n0    <|user|>67+88<|end|>\\n<|assistant|>67+88=155<|...\n1    <|user|>35+47<|end|>\\n<|assistant|>35+47=82<|e...\n2    <|user|>58+99<|end|>\\n<|assistant|>58+99=157<|...\n3    <|user|>97+47<|end|>\\n<|assistant|>97+47=144<|...\n4     <|user|>89+8<|end|>\\n<|assistant|>89+8=97<|end|>\n..                                                 ...\n245     <|user|>4*6<|end|>\\n<|assistant|>4*6=24<|end|>\n246     <|user|>8*2<|end|>\\n<|assistant|>8*2=16<|end|>\n247   <|user|>4*10<|end|>\\n<|assistant|>4*10=40<|end|>\n248     <|user|>3*7<|end|>\\n<|assistant|>3*7=21<|end|>\n249     <|user|>9*4<|end|>\\n<|assistant|>9*4=36<|end|>\n\n[250 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|user|&gt;67+88&lt;|end|&gt;\\n&lt;|assistant|&gt;67+88=155&lt;|...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|user|&gt;35+47&lt;|end|&gt;\\n&lt;|assistant|&gt;35+47=82&lt;|e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|user|&gt;58+99&lt;|end|&gt;\\n&lt;|assistant|&gt;58+99=157&lt;|...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|user|&gt;97+47&lt;|end|&gt;\\n&lt;|assistant|&gt;97+47=144&lt;|...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|user|&gt;89+8&lt;|end|&gt;\\n&lt;|assistant|&gt;89+8=97&lt;|end|&gt;</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>&lt;|user|&gt;4*6&lt;|end|&gt;\\n&lt;|assistant|&gt;4*6=24&lt;|end|&gt;</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>&lt;|user|&gt;8*2&lt;|end|&gt;\\n&lt;|assistant|&gt;8*2=16&lt;|end|&gt;</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>&lt;|user|&gt;4*10&lt;|end|&gt;\\n&lt;|assistant|&gt;4*10=40&lt;|end|&gt;</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>&lt;|user|&gt;3*7&lt;|end|&gt;\\n&lt;|assistant|&gt;3*7=21&lt;|end|&gt;</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>&lt;|user|&gt;9*4&lt;|end|&gt;\\n&lt;|assistant|&gt;9*4=36&lt;|end|&gt;</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #### Project Config\n#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\nproject_name = 'phi-ne-tuning-1-4' # @param {type:\"string\"}\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct' # @param {type:\"string\"}\n\n#@markdown ---\n#@markdown #### Push to Hub?\n#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n#@markdown You can find your token here: https://huggingface.co/settings/tokens\npush_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\nhf_token = \"\" #@param {type:\"string\"}\n#repo_id = \"styalai/phine_tuning_1\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown #### Hyperparameters\nlearning_rate = 3e-4 # @param {type:\"number\"}\nnum_epochs = 1 #@param {type:\"number\"}\nbatch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\nblock_size = 5 # @param {type:\"number\"}\ntrainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\nwarmup_ratio = 0.1 # @param {type:\"number\"}\nweight_decay = 0.01 # @param {type:\"number\"}\ngradient_accumulation = 4 # @param {type:\"number\"}\nmixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\npeft = True # @param [\"False\", \"True\"] {type:\"raw\"}\nquantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\nlora_r = 16 #@param {type:\"number\"}\nlora_alpha = 32 #@param {type:\"number\"}\nlora_dropout = 0.05 #@param {type:\"number\"}\n\nos.environ[\"PROJECT_NAME\"] = project_name\nos.environ[\"MODEL_NAME\"] = model_name\nos.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\nos.environ[\"HF_TOKEN\"] = hf_token\n#os.environ[\"REPO_ID\"] = repo_id\nos.environ[\"LEARNING_RATE\"] = str(learning_rate)\nos.environ[\"NUM_EPOCHS\"] = str(num_epochs)\nos.environ[\"BATCH_SIZE\"] = str(batch_size)\nos.environ[\"BLOCK_SIZE\"] = str(block_size)\nos.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\nos.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\nos.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\nos.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\nos.environ[\"PEFT\"] = str(peft)\nos.environ[\"QUANTIZATION\"] = str(quantization)\nos.environ[\"LORA_R\"] = str(lora_r)\nos.environ[\"LORA_ALPHA\"] = str(lora_alpha)\nos.environ[\"LORA_DROPOUT\"] = str(lora_dropout)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:50:50.704544Z","iopub.execute_input":"2024-04-30T11:50:50.704946Z","iopub.status.idle":"2024-04-30T11:50:50.718468Z","shell.execute_reply.started":"2024-04-30T11:50:50.704913Z","shell.execute_reply":"2024-04-30T11:50:50.717341Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"num sample / block size must be > 50","metadata":{}},{"cell_type":"code","source":"!autotrain llm -help","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!autotrain llm \\\n--train \\\n--username \"styalai\" \\\n--merge-adapter \\\n--model ${MODEL_NAME} \\\n--project-name ${PROJECT_NAME} \\\n--data-path data/ \\\n--text-column text \\\n--lr ${LEARNING_RATE} \\\n--batch-size ${BATCH_SIZE} \\\n--epochs ${NUM_EPOCHS} \\\n--block-size ${BLOCK_SIZE} \\\n--warmup-ratio ${WARMUP_RATIO} \\\n--lora-r ${LORA_R} \\\n--lora-alpha ${LORA_ALPHA} \\\n--lora-dropout ${LORA_DROPOUT} \\\n--weight-decay ${WEIGHT_DECAY} \\\n--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n--quantization ${QUANTIZATION} \\\n--mixed-precision ${MIXED_PRECISION} \\\n$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:50:51.912418Z","iopub.execute_input":"2024-04-30T11:50:51.913328Z","iopub.status.idle":"2024-04-30T12:06:23.652942Z","shell.execute_reply.started":"2024-04-30T11:50:51.913288Z","shell.execute_reply":"2024-04-30T12:06:23.651616Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:50:59\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-04-30 11:50:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train, deploy, func, version, inference, backend\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:50:59\u001b[0m | \u001b[36mautotrain.backend\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:50:59\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'phi-ne-tuning-1-4/training_params.json']\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:50:59\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1m{'model': 'microsoft/Phi-3-mini-4k-instruct', 'project_name': 'phi-ne-tuning-1-4', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 5, 'model_max_length': 1024, 'padding': None, 'trainer': 'default', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0003, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': True, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'text', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'styalai', 'token': '*****'}\u001b[0m\n/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\nGenerating train split: 250 examples [00:00, 37858.83 examples/s]\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:14\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mTrain data: Dataset({\n    features: ['text'],\n    num_rows: 250\n})\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:14\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mValid data: None\u001b[0m\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:15\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mcreating training arguments...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:15\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:15\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m269\u001b[0m - \u001b[1mloading model config...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:15\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1mloading model...\u001b[0m\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.17s/it]\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:28\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:28\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mpreparing peft model...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:28\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mUsing block size 5\u001b[0m\nRunning tokenizer on train dataset: 100%|█| 250/250 [00:00<00:00, 15797.52 examp\nGrouping texts in chunks of 5 (num_proc=4): 100%|█| 250/250 [00:00<00:00, 1076.6\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:51:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n 10%|████▎                                     | 25/247 [01:07<09:51,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:52:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 6.3125, 'grad_norm': 14.119974136352539, 'learning_rate': 0.00025199999999999995, 'epoch': 0.10121457489878542}\u001b[0m\n{'loss': 6.3125, 'grad_norm': 14.119974136352539, 'learning_rate': 0.00025199999999999995, 'epoch': 0.1}\n 20%|████████▌                                 | 50/247 [02:14<08:44,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:53:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 2.1156, 'grad_norm': 4.291274547576904, 'learning_rate': 0.0002716216216216216, 'epoch': 0.20242914979757085}\u001b[0m\n{'loss': 2.1156, 'grad_norm': 4.291274547576904, 'learning_rate': 0.0002716216216216216, 'epoch': 0.2}\n 30%|████████████▊                             | 75/247 [03:20<07:37,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:54:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.8421, 'grad_norm': 3.065535068511963, 'learning_rate': 0.0002378378378378378, 'epoch': 0.30364372469635625}\u001b[0m\n{'loss': 1.8421, 'grad_norm': 3.065535068511963, 'learning_rate': 0.0002378378378378378, 'epoch': 0.3}\n 40%|████████████████▌                        | 100/247 [04:27<06:30,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:55:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.5862, 'grad_norm': 9.680872917175293, 'learning_rate': 0.00020405405405405405, 'epoch': 0.4048582995951417}\u001b[0m\n{'loss': 1.5862, 'grad_norm': 9.680872917175293, 'learning_rate': 0.00020405405405405405, 'epoch': 0.4}\n 51%|████████████████████▋                    | 125/247 [05:33<05:24,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:57:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.6413, 'grad_norm': 2.9506890773773193, 'learning_rate': 0.0001716216216216216, 'epoch': 0.5060728744939271}\u001b[0m\n{'loss': 1.6413, 'grad_norm': 2.9506890773773193, 'learning_rate': 0.0001716216216216216, 'epoch': 0.51}\n 61%|████████████████████████▉                | 150/247 [06:40<04:18,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:58:09\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.4961, 'grad_norm': 3.209545850753784, 'learning_rate': 0.00013783783783783782, 'epoch': 0.6072874493927125}\u001b[0m\n{'loss': 1.4961, 'grad_norm': 3.209545850753784, 'learning_rate': 0.00013783783783783782, 'epoch': 0.61}\n 71%|█████████████████████████████            | 175/247 [07:46<03:11,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:59:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.3884, 'grad_norm': 5.435689449310303, 'learning_rate': 0.00010405405405405404, 'epoch': 0.708502024291498}\u001b[0m\n{'loss': 1.3884, 'grad_norm': 5.435689449310303, 'learning_rate': 0.00010405405405405404, 'epoch': 0.71}\n 81%|█████████████████████████████████▏       | 200/247 [08:53<02:04,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:00:22\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.4064, 'grad_norm': 5.001658916473389, 'learning_rate': 7.027027027027026e-05, 'epoch': 0.8097165991902834}\u001b[0m\n{'loss': 1.4064, 'grad_norm': 5.001658916473389, 'learning_rate': 7.027027027027026e-05, 'epoch': 0.81}\n 91%|█████████████████████████████████████▎   | 225/247 [09:59<00:58,  2.67s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:01:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.3423, 'grad_norm': 2.2748093605041504, 'learning_rate': 3.648648648648649e-05, 'epoch': 0.9109311740890689}\u001b[0m\n{'loss': 1.3423, 'grad_norm': 2.2748093605041504, 'learning_rate': 3.648648648648649e-05, 'epoch': 0.91}\n100%|█████████████████████████████████████████| 247/247 [10:58<00:00,  2.66s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:02:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'train_runtime': 658.0757, 'train_samples_per_second': 1.501, 'train_steps_per_second': 0.375, 'train_loss': 2.066188132714646, 'epoch': 1.0}\u001b[0m\n{'train_runtime': 658.0757, 'train_samples_per_second': 1.501, 'train_steps_per_second': 0.375, 'train_loss': 2.066188132714646, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 247/247 [10:58<00:00,  2.66s/it]\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:02:28\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:02:30\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[1mMerging adapter weights...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:02:30\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mmerge_adapter\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mLoading adapter...\u001b[0m\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.27s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:03:35\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mmerge_adapter\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mSaving target model...\u001b[0m\n\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 12:04:01\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mPushing model to hub...\u001b[0m\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n\nUpload 4 LFS files:   0%|                                 | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n\ntokenizer.model:   0%|                               | 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00001-of-00002.safetensors:   0%|     | 573k/4.97G [00:00<14:39, 5.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\ntraining_args.bin: 100%|███████████████████| 4.98k/4.98k [00:00<00:00, 32.9kB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 15.5M/4.97G [00:00<00:55, 89.9MB/s]\nmodel-00001-of-00002.safetensors:   0%|    | 24.6M/4.97G [00:00<01:25, 57.7MB/s]\u001b[A\ntokenizer.model: 100%|████████████████████████| 500k/500k [00:00<00:00, 864kB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 32.0M/4.97G [00:00<02:00, 40.9MB/s]\nmodel-00002-of-00002.safetensors:   1%|    | 37.0M/2.67G [00:00<00:54, 48.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 59.6M/4.97G [00:01<01:23, 59.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 66.8M/4.97G [00:01<01:56, 42.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   2%|    | 64.0M/2.67G [00:01<01:08, 37.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|    | 93.0M/4.97G [00:01<01:26, 56.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|     | 100M/4.97G [00:02<01:48, 45.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|     | 112M/4.97G [00:02<01:52, 43.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   4%|▏    | 109M/2.67G [00:02<00:54, 47.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 128M/4.97G [00:02<01:38, 49.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 144M/4.97G [00:02<01:25, 56.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 160M/4.97G [00:03<01:21, 58.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   5%|▎    | 141M/2.67G [00:03<00:53, 47.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 176M/4.97G [00:03<01:27, 55.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 195M/4.97G [00:03<01:27, 54.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 208M/4.97G [00:04<01:35, 49.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   6%|▎    | 173M/2.67G [00:04<01:04, 39.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▏    | 224M/4.97G [00:04<01:36, 49.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   7%|▎    | 189M/2.67G [00:04<00:53, 46.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▏    | 240M/4.97G [00:04<01:32, 51.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎    | 256M/4.97G [00:04<01:25, 55.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎    | 272M/4.97G [00:05<01:25, 54.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   8%|▍    | 224M/2.67G [00:05<01:08, 35.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 336M/4.97G [00:06<01:40, 46.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   9%|▍    | 242M/2.67G [00:06<02:37, 15.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 352M/4.97G [00:07<02:11, 35.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▍    | 259M/2.67G [00:07<01:48, 22.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 368M/4.97G [00:07<01:49, 42.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▌    | 276M/2.67G [00:07<01:26, 27.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 384M/4.97G [00:07<01:44, 43.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 400M/4.97G [00:08<01:38, 46.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  11%|▌    | 304M/2.67G [00:08<01:11, 33.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  12%|▌    | 318M/2.67G [00:08<00:49, 47.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 416M/4.97G [00:08<02:12, 34.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▋    | 335M/2.67G [00:08<00:47, 49.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 448M/4.97G [00:09<01:48, 41.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▋    | 352M/2.67G [00:09<01:08, 34.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 464M/4.97G [00:09<01:33, 48.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  14%|▋    | 370M/2.67G [00:09<01:00, 38.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▍    | 480M/4.97G [00:09<01:33, 47.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▍    | 496M/4.97G [00:10<01:27, 51.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌    | 512M/4.97G [00:10<01:25, 51.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  15%|▊    | 413M/2.67G [00:10<00:41, 54.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 528M/4.97G [00:10<01:36, 45.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 544M/4.97G [00:11<01:28, 50.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  16%|▊    | 436M/2.67G [00:11<00:53, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 576M/4.97G [00:11<01:20, 54.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 588M/4.97G [00:11<01:14, 58.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  17%|▊    | 464M/2.67G [00:12<01:04, 34.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 594M/4.97G [00:12<01:33, 47.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 623M/4.97G [00:12<01:18, 55.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 630M/4.97G [00:12<01:27, 49.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 640M/4.97G [00:13<01:31, 47.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 656M/4.97G [00:13<01:32, 46.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 672M/4.97G [00:13<01:20, 53.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  21%|█    | 560M/2.67G [00:13<00:39, 52.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 688M/4.97G [00:14<01:29, 47.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 704M/4.97G [00:14<01:28, 48.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▋    | 736M/4.97G [00:14<01:20, 52.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊    | 752M/4.97G [00:15<01:15, 55.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 784M/4.97G [00:15<01:10, 59.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  24%|█▏   | 640M/2.67G [00:15<00:47, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 816M/4.97G [00:16<01:10, 58.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  25%|█▎   | 672M/2.67G [00:16<00:39, 50.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 848M/4.97G [00:16<01:09, 59.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 864M/4.97G [00:17<01:04, 63.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  27%|█▎   | 720M/2.67G [00:17<00:36, 53.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 880M/4.97G [00:17<01:06, 61.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 896M/4.97G [00:17<01:07, 60.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  28%|█▍   | 752M/2.67G [00:17<00:34, 55.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 928M/4.97G [00:18<01:01, 65.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  29%|█▍   | 771M/2.67G [00:18<00:38, 49.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 944M/4.97G [00:18<01:04, 62.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 960M/4.97G [00:18<01:05, 61.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▉    | 976M/4.97G [00:18<01:05, 61.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▉    | 992M/4.97G [00:19<01:03, 62.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▊   | 1.01G/4.97G [00:19<00:59, 66.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|▊   | 1.04G/4.97G [00:19<00:55, 70.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  32%|█▌   | 864M/2.67G [00:19<00:34, 52.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  33%|█▋   | 880M/2.67G [00:20<00:32, 55.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 896M/2.67G [00:20<00:31, 56.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 912M/2.67G [00:20<00:27, 64.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  35%|█▋   | 928M/2.67G [00:20<00:27, 63.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  35%|█▊   | 944M/2.67G [00:21<00:28, 61.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|▉   | 1.10G/4.97G [00:22<01:38, 39.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|▉   | 1.12G/4.97G [00:22<01:28, 43.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  37%|█▊   | 989M/2.67G [00:22<00:50, 33.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|▉   | 1.14G/4.97G [00:23<01:28, 43.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|▉   | 1.15G/4.97G [00:23<01:17, 49.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|▉   | 1.17G/4.97G [00:23<01:09, 55.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|▉   | 1.18G/4.97G [00:23<01:08, 55.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  39%|█▌  | 1.04G/2.67G [00:23<00:37, 43.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|▉   | 1.20G/4.97G [00:24<01:11, 52.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|▉   | 1.22G/4.97G [00:24<01:03, 59.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|▉   | 1.23G/4.97G [00:24<01:05, 57.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█   | 1.25G/4.97G [00:25<01:06, 56.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█   | 1.26G/4.97G [00:25<01:00, 61.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.30G/4.97G [00:25<00:56, 65.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.31G/4.97G [00:25<00:55, 66.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.33G/4.97G [00:26<00:57, 63.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.34G/4.97G [00:26<00:54, 66.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█   | 1.38G/4.97G [00:26<00:54, 66.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█   | 1.39G/4.97G [00:27<00:54, 65.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▏  | 1.41G/4.97G [00:27<00:56, 63.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.42G/4.97G [00:27<00:55, 63.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.44G/4.97G [00:27<00:55, 63.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.46G/4.97G [00:28<01:01, 56.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.47G/4.97G [00:28<01:10, 49.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.49G/4.97G [00:28<01:06, 52.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.51G/4.97G [00:29<00:57, 60.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▏  | 1.55G/4.97G [00:30<00:57, 59.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  51%|██  | 1.36G/2.67G [00:30<00:32, 40.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▎  | 1.58G/4.97G [00:30<00:58, 58.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  52%|██  | 1.39G/2.67G [00:30<00:26, 47.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.62G/4.97G [00:31<01:27, 38.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.63G/4.97G [00:32<01:14, 44.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.65G/4.97G [00:32<01:06, 49.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.66G/4.97G [00:32<01:00, 54.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▎  | 1.68G/4.97G [00:32<01:00, 54.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▎  | 1.70G/4.97G [00:32<00:55, 59.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▍  | 1.71G/4.97G [00:33<00:54, 59.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  57%|██▎ | 1.52G/2.67G [00:33<00:19, 58.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.74G/4.97G [00:33<00:56, 57.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.76G/4.97G [00:34<00:56, 56.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▎ | 1.57G/2.67G [00:34<00:19, 56.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▎ | 1.58G/2.67G [00:34<00:18, 58.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  60%|██▍ | 1.60G/2.67G [00:34<00:18, 57.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  61%|██▍ | 1.62G/2.67G [00:34<00:17, 60.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  61%|██▍ | 1.63G/2.67G [00:35<00:18, 55.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  62%|██▍ | 1.65G/2.67G [00:35<00:17, 58.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  62%|██▍ | 1.66G/2.67G [00:35<00:16, 61.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  63%|██▌ | 1.68G/2.67G [00:35<00:14, 66.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  64%|██▌ | 1.70G/2.67G [00:36<00:15, 63.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  64%|██▌ | 1.71G/2.67G [00:36<00:15, 61.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  65%|██▌ | 1.73G/2.67G [00:36<00:16, 56.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  65%|██▌ | 1.74G/2.67G [00:37<00:16, 54.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  66%|██▋ | 1.76G/2.67G [00:37<00:17, 51.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  67%|██▋ | 1.78G/2.67G [00:37<00:17, 52.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  67%|██▋ | 1.79G/2.67G [00:38<00:19, 45.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  68%|██▋ | 1.81G/2.67G [00:38<00:16, 52.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.78G/4.97G [00:38<05:30, 9.67MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.79G/4.97G [00:39<04:00, 13.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.81G/4.97G [00:39<02:51, 18.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.82G/4.97G [00:39<02:06, 24.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  71%|██▊ | 1.89G/2.67G [00:39<00:13, 55.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.84G/4.97G [00:40<01:53, 27.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.87G/4.97G [00:40<01:18, 39.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  73%|██▉ | 1.94G/2.67G [00:40<00:14, 51.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.89G/4.97G [00:41<01:14, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.90G/4.97G [00:41<01:08, 44.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.92G/4.97G [00:41<01:02, 48.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.95G/4.97G [00:42<00:53, 56.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.97G/4.97G [00:42<00:55, 54.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.98G/4.97G [00:42<00:53, 55.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 2.00G/4.97G [00:42<00:52, 57.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▌  | 2.02G/4.97G [00:43<00:55, 53.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▋  | 2.03G/4.97G [00:43<00:53, 55.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▋  | 2.05G/4.97G [00:43<00:50, 57.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 2.06G/4.97G [00:44<00:53, 54.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  80%|███▏| 2.13G/2.67G [00:44<00:08, 61.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  80%|███▏| 2.14G/2.67G [00:44<00:09, 56.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  81%|███▏| 2.16G/2.67G [00:44<00:09, 54.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  82%|███▎| 2.18G/2.67G [00:45<00:09, 54.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  82%|███▎| 2.19G/2.67G [00:45<00:08, 55.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  83%|███▎| 2.21G/2.67G [00:45<00:08, 52.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  83%|███▎| 2.22G/2.67G [00:46<00:08, 53.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  84%|███▎| 2.24G/2.67G [00:46<00:08, 51.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  85%|███▍| 2.26G/2.67G [00:46<00:07, 54.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  85%|███▍| 2.27G/2.67G [00:47<00:07, 52.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 2.09G/4.97G [00:47<02:54, 16.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 2.10G/4.97G [00:47<02:35, 18.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 2.11G/4.97G [00:47<01:55, 24.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 2.11G/4.97G [00:48<02:02, 23.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 2.13G/4.97G [00:48<01:34, 30.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  89%|███▌| 2.37G/2.67G [00:48<00:04, 67.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  89%|███▌| 2.38G/2.67G [00:48<00:04, 66.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 2.14G/4.97G [00:49<01:58, 23.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  91%|███▌| 2.42G/2.67G [00:49<00:04, 63.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 2.16G/4.97G [00:49<01:40, 27.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|█▊  | 2.18G/4.97G [00:49<01:19, 35.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|█▊  | 2.24G/4.97G [00:50<00:49, 55.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|█▊  | 2.27G/4.97G [00:51<00:44, 60.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|█▊  | 2.29G/4.97G [00:51<00:42, 62.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  94%|███▊| 2.51G/2.67G [00:51<00:03, 43.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▊  | 2.32G/4.97G [00:52<00:45, 58.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  95%|███▊| 2.54G/2.67G [00:52<00:02, 47.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▉  | 2.34G/4.97G [00:52<00:54, 48.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▉  | 2.35G/4.97G [00:52<00:49, 52.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 2.37G/4.97G [00:53<00:51, 50.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  98%|███▉| 2.61G/2.67G [00:53<00:01, 54.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 2.40G/4.97G [00:53<00:46, 55.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|█▉  | 2.42G/4.97G [00:53<00:43, 59.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors: 100%|████| 2.67G/2.67G [00:54<00:00, 49.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|████| 4.97G/4.97G [01:44<00:00, 47.8MB/s]\n\n\nUpload 4 LFS files: 100%|█████████████████████████| 4/4 [01:44<00:00, 26.09s/it]\u001b[A\u001b[A\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}